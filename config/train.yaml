path:
  ckpt_path: "./output/ckpt/custom"   # 모델 체크포인트가 저장될 경로 (custom 프로젝트로 변경)
  log_path: "./output/log/custom"     # 로그 파일이 저장될 경로
  result_path: "./output/result/custom" # 학습 결과가 저장될 경로

optimizer:
  batch_size: 1   # 배치 크기
  betas: [0.9, 0.98]  # Adam optimizer의 beta 값
  eps: 0.000000001  # epsilon 값 (숫자가 매우 작은 경우 안정적인 수렴을 위해 필요)
  weight_decay: 0.0  # 가중치 감소
  grad_clip_thresh: 1.0  # Gradient clipping 임계값
  grad_acc_step: 1  # Gradient Accumulation 스텝 수
  warm_up_step: 4000  # 학습 초기에 학습률을 점진적으로 증가시키는 스텝
  anneal_steps: [300000, 400000, 500000]  # 학습률을 감소시키는 시점
  anneal_rate: 0.3  # 학습률 감소율

step:
  total_step: 900000  # 총 학습 스텝 수
  log_step: 100  # 로그를 출력할 스텝 간격
  synth_step: 1000  # 학습 도중 샘플을 합성하여 저장할 스텝 간격
  val_step: 1000  # 검증 데이터를 사용할 스텝 간격
  save_step: 100000  # 체크포인트를 저장할 스텝 간격
